<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI Reasearch Blog</title><description>AI Reasearch Blog @ RongshengWang</description><link>https://ai-reaserch-blog.github.io/</link><language>en-US</language><item><title>GitHub Commits 标准提交规范</title><link>https://ai-reaserch-blog.github.io/posts/GitHub%20Commits%20%E6%A0%87%E5%87%86%E6%8F%90%E4%BA%A4%E8%A7%84%E8%8C%83/</link><guid isPermaLink="true">https://ai-reaserch-blog.github.io/posts/GitHub%20Commits%20%E6%A0%87%E5%87%86%E6%8F%90%E4%BA%A4%E8%A7%84%E8%8C%83/</guid><description>Conventional Commits 规范是一种轻量化的 Github Commits 提交消息约定，通过结构化格式确保提交历史清晰明确，便于自动化工具处理。</description><pubDate>Thu, 24 Apr 2025 00:00:00 GMT</pubDate><content:encoded>Conventional Commits 是一种规范化的 Git 提交信息格式。遵循规范可以有效提升 Commit Message 的可读性，也方便历史记录和版本控制。

最基础的格式如下：

```swift
1. &lt;type&gt;[optional scope]: &lt;description&gt;
2.
3. [optional body]
4.
5. [optional footer(s)]
```

其中 `&lt;type&gt;` 表示提交类型（必填），也决定它在 changelog 中的分类，通常有以下几种：

- `feat`: 新功能；
- `fix`: 修复 bug；
- `docs`: 仅修改文档；
- `style`: 不影响代码逻辑的修改，比如格式、空格、缩进、缺失的分号；
- `refactor`: 代码重构（不包含功能变更或 bug 修复）；
- `perf`: 性能优化；
- `test`: 添加测试或修改测试；
- `build`: 构建系统或依赖的变动（例如 webpack、rollup）；
- `chore`: 杂项、不属于其他类型的更改（比如改 .gitignore、更新依赖）；
- `ci`: 持续集成相关（GitHub Actions、Travis CI、Circle 等）；
- `revert`: 回滚某个提交（会自动生成 footer）。

而 `[optional scope]` 表示影响范围（可选），`[description]` 表示简短的描述（必填），例如：

```shell
1. feat(blog): add comment system
2. fix(pdf): correct link to external document
3. docs(readme): update usage instructions
```

`[optional body]` 在解释提交的动机、问题背景、修改细节，尤其是重大提交或 refactor 时有用。`[optional footer(s)]` 则用于关闭 issue、BREAKING CHANGE 等信息。

本文更多参考：[Conventional Commits 1.0.0](https://www.conventionalcommits.org/en/v1.0.0/)</content:encoded></item><item><title>Sky-T1: Train your own O1 preview model within $450</title><link>https://ai-reaserch-blog.github.io/posts/sky-t1/</link><guid isPermaLink="true">https://ai-reaserch-blog.github.io/posts/sky-t1/</guid><description>We introduce Sky-T1-32B-Preview, our reasoning model that performs on par with o1-preview on popular reasoning and coding benchmarks.</description><pubDate>Fri, 10 Jan 2025 00:00:00 GMT</pubDate><content:encoded>We introduce Sky-T1-32B-Preview, our reasoning model that performs on par with o1-preview on popular reasoning and coding benchmarks. **Remarkably, Sky-T1-32B-Preview was trained for less than $450, demonstrating that it is possible to replicate high-level reasoning capabilities affordably and efficiently.** All [code](https://github.com/NovaSky-AI/SkyThought) is open-source. 

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1/Sky-T1-pipeline.jpg)

## Overview
Models such as o1 and Gemini 2.0 flash thinking that excel in reasoning have shown to solve complex tasks by producing a long internal chain of thought, among other advancements. However, the technical details and model weights are un-accessible, presenting a barrier to the participation of the academic and open-source communities.

In response, a few notable efforts have emerged to train open-weight reasoning models in the math domain, such as [Still-2](https://arxiv.org/abs/2412.09413) and [Journey](https://arxiv.org/abs/2411.16489). Concurrently, we, the NovaSky team at UC Berkeley, have been exploring various techniques to evolve the reasoning capabilities of base and instruct-tuned models. In this work, we achieve competitive reasoning performance not just in math, but also in coding in the same model.

### Fully Open-source: Driving Progress Together
To ensure our work benefits the broader community, we are fully committed to open-source collaboration. We open-source all details (i.e., data, codes, model weights) to enable the community to replicate and improve on our results *easily*:
 - [**Infrastructure**](https://github.com/NovaSky-AI/SkyThought): to build the data, train, and evaluate the model in a single repository.
 - [**Data**](https://github.com/NovaSky-AI/SkyThought): 17K data used to train Sky-T1-32B-Preview.
 - [**Technical details**](https://novasky-ai.github.io/posts/sky-t1): Our technical [report](https://novasky-ai.github.io/posts/sky-t1/) with a [wandb log](https://api.wandb.ai/links/sky-posttraining-uc-berkeley/wjg3sybl).
 - [**Model weights**](https://huggingface.co/NovaSky-AI): Our 32B model weight.

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;background-color: #bfbfbf;&quot;&gt;&lt;div align=&quot;center&quot;&gt;Sky-T1-32B-Preview&lt;/div&gt;&lt;/th&gt;
      &lt;th&gt;&lt;div align=&quot;center&quot;&gt;STILL-2&lt;/div&gt;&lt;/th&gt;
      &lt;th&gt;&lt;div align=&quot;center&quot;&gt;Journey&lt;/div&gt;&lt;/th&gt;
      &lt;th&gt;&lt;div align=&quot;center&quot;&gt;QwQ&lt;/div&gt;&lt;/th&gt;
      &lt;th&gt;&lt;div align=&quot;center&quot;&gt;o1&lt;/div&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Data&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Code&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Report&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Math Domain&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Coding Domain&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Weights&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

By sharing all these resources, we aim to empower the academic and open-source communities to build on our work, explore new possibilities, and push the boundaries of reasoning model development.

## Recipes
### Data Curation Process
To generate our training data we use QwQ-32B-Preview, an open-source model with reasoning capabilities comparable to o1-preview. We curate the data mixture  (see later section) to cover diverse domains that require reasoning, and a reject sampling procedure to improve the data quality. We then rewrite QwQ traces with GPT-4o-mini into a well-formatted version, inspired by [Still-2](https://arxiv.org/abs/2412.09413), to improve data quality and ease parsing. We particularly find the ease of parsing advantageous for reasoning models - they are trained to respond in a particular format, where results are often hard to parse. For instance, on the APPs dataset, without reformatting, we can only assume that the code is written in the last code block, where QwQ only achieves ~25% accuracy. However, sometimes code can be written in the middle, where after reformatting, the accuracy is boosted to higher than 90%.

**Rejection Sampling:** We discard QwQ samples if they are incorrect according to the solutions provided in datasets. For Math problems, we do exact matching with the ground truth solutions. For coding problems, we execute the unit tests provided in datasets. Our final data contains 5k coding data from APPs and TACO, and 10k math data from AIME, MATH, and Olympiads subsets of the NuminaMATH dataset. In addition, we maintain 1k science and puzzle data from STILL-2.

### Training
We use our training data to fine tune Qwen2.5-32B-Instruct, an open source model without reasoning capabilities. The model is trained with 3 epochs, learning rate 1e-5 and batch size 96. The model training finishes in 19 hours on 8 H100 with DeepSpeed Zero-3 offload (~ $450 according to Lambda Cloud pricing). We use [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) to perform training.

### Evaluation and Results
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th style=&quot;background-color: #bfbfbf;&quot;&gt;Sky-T1-32B-Preview&lt;/th&gt;
      &lt;th&gt;Qwen-2.5-32B-Instruct&lt;/th&gt;
      &lt;th&gt;QwQ&lt;/th&gt;
      &lt;th&gt;o1-preview&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Math500&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;82.4&lt;/td&gt;
      &lt;td&gt;76.2&lt;/td&gt;
      &lt;td&gt;85.4&lt;/td&gt;
      &lt;td&gt;81.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AIME2024&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;43.3&lt;/td&gt;
      &lt;td&gt;16.7&lt;/td&gt;
      &lt;td&gt;50.0&lt;/td&gt;
      &lt;td&gt;40.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LiveCodeBench-Easy&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;86.3&lt;/td&gt;
      &lt;td&gt;84.6&lt;/td&gt;
      &lt;td&gt;90.7&lt;/td&gt;
      &lt;td&gt;92.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LiveCodeBench-Medium&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;56.8&lt;/td&gt;
      &lt;td&gt;40.8&lt;/td&gt;
      &lt;td&gt;56.3&lt;/td&gt;
      &lt;td&gt;54.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LiveCodeBench-Hard&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;17.9&lt;/td&gt;
      &lt;td&gt;9.8&lt;/td&gt;
      &lt;td&gt;17.1&lt;/td&gt;
      &lt;td&gt;16.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPQA-Diamond&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;56.8&lt;/td&gt;
      &lt;td&gt;45.5&lt;/td&gt;
      &lt;td&gt;52.5&lt;/td&gt;
      &lt;td&gt;75.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;


## Other findings
**Model size matters.** We initially experimented with training on smaller models (7B and 14B) but observed only modest improvements. For example, training Qwen2.5-14B-Coder-Instruct on the APPs dataset resulted in a slight performance increase on LiveCodeBench from 42.6% to 46.3%. However, upon manually inspecting outputs from smaller models (those smaller than 32B), we found that they frequently generated repetitive content, limiting their effectiveness.


**Data mixture matters.** We initially trained a 32B model using 3–4K math problems from the Numina dataset (provided by STILL-2), achieving a significant improvement in AIME24 accuracy from 16.7% to 43.3%. However, when we incorporated coding data generated from the APPs dataset into the training process, AIME24 accuracy dropped to 36.7%. We hypothesize that this decline is due to the distinct reasoning approaches required for math and coding tasks.

Reasoning in coding often involves additional logical steps, such as simulating test inputs or internally executing generated code, whereas reasoning for math problems tends to be more direct and structured. To address these differences, we enriched the training data with challenging math problems from the NuminaMath dataset and complex coding tasks from the TACO dataset. This balanced data mixture enabled the model to excel in both domains, restoring 43.3% accuracy on AIME24 while also improving its coding capabilities.

## Future work
Sky-T1-32B-Preview marks the start of our journey to develop open-sourced models with advanced reasoning capabilities. Moving forward, we will focus on developing more efficient models that maintain strong reasoning performance and exploring advanced techniques that further enhance the models’ efficiency and accuracy at test time. Stay tuned as we make progress on these exciting initiatives.


## Acknowledgement
This work is done at [Berkeley Sky Computing Lab](https://sky.cs.berkeley.edu/), with the amazing compute support from [Lambda Labs](https://lambdalabs.com/service/gpu-cloud?srsltid=AfmBOop5FnmEFTkavVtdZDsLWvHWNg6peXtat-OXJ9MW5GMNsk756PE5) and [Anyscale](https://www.anyscale.com/). We would like to express our gratitude for the valuable academic feedback and support from the [Still-2 Team](https://arxiv.org/pdf/2412.09413), and Junyang Lin from the [Qwen Team](https://qwenlm.github.io/).

## Citation
```bibtex
@misc{sky_t1_2025,
  author       = {NovaSky Team},
  title        = {Sky-T1: Train your own O1 preview model within $450},
  howpublished = {https://novasky-ai.github.io/posts/sky-t1},
  note         = {Accessed: 2025-01-09},
  year         = {2025}
}</content:encoded></item></channel></rss>